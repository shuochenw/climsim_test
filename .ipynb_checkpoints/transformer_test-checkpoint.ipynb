{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f978ab-88d5-4a73-bf37-e2ea9676c50d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:20:31.474151Z",
     "iopub.status.busy": "2024-07-22T03:20:31.470838Z",
     "iopub.status.idle": "2024-07-22T03:20:54.347839Z",
     "shell.execute_reply": "2024-07-22T03:20:54.343810Z",
     "shell.execute_reply.started": "2024-07-22T03:20:31.474013Z"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "846e4cb0-9cfb-4573-9d5b-ef74dd68e90d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:20:54.360817Z",
     "iopub.status.busy": "2024-07-22T03:20:54.358599Z",
     "iopub.status.idle": "2024-07-22T03:20:56.719212Z",
     "shell.execute_reply": "2024-07-22T03:20:56.714595Z",
     "shell.execute_reply.started": "2024-07-22T03:20:54.360676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tesla V100-PCIE-32GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a99b53-4c8f-4d5f-95e2-7ea608f7accd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:20:56.728060Z",
     "iopub.status.busy": "2024-07-22T03:20:56.726429Z",
     "iopub.status.idle": "2024-07-22T03:20:56.749013Z",
     "shell.execute_reply": "2024-07-22T03:20:56.745514Z",
     "shell.execute_reply.started": "2024-07-22T03:20:56.727911Z"
    }
   },
   "outputs": [],
   "source": [
    "SEQUENCE_SIZE = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 20\n",
    "IN_FEATURES = 124\n",
    "OUT_FEATURES = 128\n",
    "# RANDOM_SEED = 42\n",
    "D_MODEL = 64\n",
    "# DROPOUT = 0.2\n",
    "N_HEAD = 4\n",
    "N_LAYER = 2\n",
    "MAX_LEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce50a24-ac7d-4b63-8386-a46dbda3b027",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:20:56.754142Z",
     "iopub.status.busy": "2024-07-22T03:20:56.752825Z",
     "iopub.status.idle": "2024-07-22T03:21:23.539302Z",
     "shell.execute_reply": "2024-07-22T03:21:23.534627Z",
     "shell.execute_reply.started": "2024-07-22T03:20:56.754014Z"
    }
   },
   "outputs": [],
   "source": [
    "# # use one year\n",
    "# X=np.load('/work/sds-lab/Shuochen/climsim/val_input.npy')\n",
    "# y=np.load('/work/sds-lab/Shuochen/climsim/val_target.npy')\n",
    "\n",
    "# use all 8 years: change path\n",
    "X_train=np.load('/work/sds-lab/Shuochen/climsim/train_input.npy')\n",
    "y_train=np.load('/work/sds-lab/Shuochen/climsim/train_target.npy')\n",
    "X_test=np.load('/work/sds-lab/Shuochen/climsim/val_input.npy')\n",
    "y_test=np.load('/work/sds-lab/Shuochen/climsim/val_target.npy')\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.Tensor(y_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_test = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c279573-e4f9-4ee8-ae0a-80748ecb9cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:21:23.555872Z",
     "iopub.status.busy": "2024-07-22T03:21:23.552114Z",
     "iopub.status.idle": "2024-07-22T03:22:57.652985Z",
     "shell.execute_reply": "2024-07-22T03:22:57.648333Z",
     "shell.execute_reply.started": "2024-07-22T03:21:23.555738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384, 26280, 124]) torch.Size([384, 26280, 128])\n",
      "torch.Size([384, 26271, 124, 10]) torch.Size([384, 26271, 128, 10])\n",
      "torch.Size([384, 26271, 10, 124]) torch.Size([384, 26271, 10, 128])\n",
      "torch.Size([10088064, 10, 124]) torch.Size([10088064, 10, 128])\n",
      "torch.Size([384, 3755, 124]) torch.Size([384, 3755, 128])\n",
      "torch.Size([384, 3746, 124, 10]) torch.Size([384, 3746, 128, 10])\n",
      "torch.Size([384, 3746, 10, 124]) torch.Size([384, 3746, 10, 128])\n",
      "torch.Size([1438464, 10, 124]) torch.Size([1438464, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "def create_sliding_window(X, y):\n",
    "    # to tensor, reshape, transpose (time * space, 124 or 128) > (space, time, 124 or 128)\n",
    "    X=X.reshape(int(X.shape[0]/384),384,124).permute(1,0,2)\n",
    "    y=y.reshape(int(y.shape[0]/384),384,128).permute(1,0,2)\n",
    "    print(X.shape,y.shape)\n",
    "    # create sliding window (space, time, 124 or 128) > (space, num_window, 124 or 128, seq_size)\n",
    "    X = X.unfold(1,SEQUENCE_SIZE,1)\n",
    "    y = y.unfold(1,SEQUENCE_SIZE,1)\n",
    "    print(X.shape,y.shape)\n",
    "    # transpose (space, num_window, 124 or 128, seq_size) > (space, num_window, seq_size, 124 or 128)\n",
    "    X = X.permute(0,1,3,2)\n",
    "    y = y.permute(0,1,3,2)\n",
    "    print(X.shape,y.shape)\n",
    "    # combine dimension (space, num_window, seq_size, 124 or 128) > (space * num_window, seq_size, 124 or 128)\n",
    "    X = X.reshape(384 * X.shape[1],SEQUENCE_SIZE,124)\n",
    "    y = y.reshape(384 * y.shape[1],SEQUENCE_SIZE,128)\n",
    "    print(X.shape,y.shape)\n",
    "    return X, y\n",
    "\n",
    "# X, y = create_sliding_window(X, y)\n",
    "X_train, y_train = create_sliding_window(X_train, y_train)\n",
    "X_test, y_test = create_sliding_window(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c5e756-12e2-428e-874b-2709d201816e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:22:57.661499Z",
     "iopub.status.busy": "2024-07-22T03:22:57.659562Z",
     "iopub.status.idle": "2024-07-22T03:22:57.690789Z",
     "shell.execute_reply": "2024-07-22T03:22:57.686876Z",
     "shell.execute_reply.started": "2024-07-22T03:22:57.661356Z"
    }
   },
   "outputs": [],
   "source": [
    "# # # split datasets to training and testing\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "# print(y_test.shape)\n",
    "\n",
    "# create datasets\n",
    "training_set = TensorDataset(X_train, y_train)\n",
    "testing_set = TensorDataset(X_test, y_test)\n",
    "# create dataloaders\n",
    "train_dataloader = DataLoader(training_set, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "test_dataloader = DataLoader(testing_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8c4a4e4-3631-455e-827b-9d6117db5699",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:22:57.699278Z",
     "iopub.status.busy": "2024-07-22T03:22:57.696157Z",
     "iopub.status.idle": "2024-07-22T03:22:57.736313Z",
     "shell.execute_reply": "2024-07-22T03:22:57.732362Z",
     "shell.execute_reply.started": "2024-07-22T03:22:57.699079Z"
    }
   },
   "outputs": [],
   "source": [
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model=D_MODEL, max_len=SEQUENCE_SIZE):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model=D_MODEL, max_len=MAX_LEN):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         # self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.pe[:x.size(0), :]\n",
    "#         # return self.dropout(x)\n",
    "#         return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f76d7c3-6a1a-4483-9957-1653c42913a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:22:57.743821Z",
     "iopub.status.busy": "2024-07-22T03:22:57.742229Z",
     "iopub.status.idle": "2024-07-22T03:22:59.456494Z",
     "shell.execute_reply": "2024-07-22T03:22:59.452806Z",
     "shell.execute_reply.started": "2024-07-22T03:22:57.743679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578624\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "FLOPs: 82575360.0, Parameters: 16320.0\n"
     ]
    }
   ],
   "source": [
    "# Model definition using Transformer\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim=IN_FEATURES, output_dim = OUT_FEATURES, d_model=D_MODEL, \n",
    "                 nhead=N_HEAD, num_layers=N_LAYER):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding()\n",
    "        # self.pos_encoder = nn.Embedding(SEQUENCE_SIZE, d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True, dropout=0.0)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, output_dim)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.decoder_2 = nn.Linear(512, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        # x = x+self.pos_encoder(torch.arange(SEQUENCE_SIZE, device=device))\n",
    "        x = self.transformer_encoder(x)\n",
    "        # x = self.decoder(x[:, -1, :])\n",
    "        x = self.decoder(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.decoder_2(x)\n",
    "        return x\n",
    "\n",
    "model = TransformerModel().to(device)\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "# Forward pass to count FLOPs\n",
    "from thop import profile\n",
    "dummy_input = torch.randn(BATCH_SIZE, SEQUENCE_SIZE, IN_FEATURES).to(device)\n",
    "flops, params = profile(model, inputs=(dummy_input,))\n",
    "print(f'FLOPs: {flops}, Parameters: {params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f8423-3a6c-4adf-9147-996a65596871",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.511Z",
     "iopub.execute_input": "2024-07-22T03:22:59.463409Z",
     "iopub.status.busy": "2024-07-22T03:22:59.461791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train loss: 0.00566 | Test loss: 0.00407\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "epochs = EPOCHS\n",
    "# early_stop_count = 0\n",
    "# min_val_loss = float('inf')\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "# this will write training and testing losses after each batch to a txt file. Delete this if not necessary.\n",
    "f = open(\"loss_Transformer.txt\", \"w\")\n",
    "# write hyperparams\n",
    "f.write(f\"SEQUENCE_SIZE={SEQUENCE_SIZE}\" + '\\t' + f\"LEARNING_RATE={LEARNING_RATE}\" + '\\t' + f\"BATCH_SIZE={BATCH_SIZE}\" + '\\t' + \n",
    "            f\"EPOCHS={EPOCHS}\" + '\\t' + f\"D_MODEL={D_MODEL}\" + '\\t' + f\"N_HEAD={N_HEAD}\" + '\\t' + f\"N_LAYER={N_LAYER}\" + '\\t' + f\"MAX_LEN={MAX_LEN}\" + '\\n')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        y_pred = model(X.to(device))\n",
    "        loss = loss_fn(y_pred, y.to(device))\n",
    "        train_loss += loss # accumulatively add up the loss per epoch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_loss_list.append(train_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Validation\n",
    "    with torch.inference_mode():\n",
    "        test_loss = 0\n",
    "        model.eval()\n",
    "        for X, y in test_dataloader:\n",
    "            test_pred = model(X.to(device))\n",
    "            test_loss += loss_fn(test_pred, y.to(device)) # accumulatively add up the loss per epoch\n",
    "        \n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_loss_list.append(test_loss.detach().cpu().numpy())\n",
    "\n",
    "    # if test_loss < min_val_loss:\n",
    "    #     min_val_loss = test_loss\n",
    "    #     early_stop_count = 0\n",
    "    # else:\n",
    "    #     early_stop_count += 1\n",
    "\n",
    "    # if early_stop_count >= 5:\n",
    "    #     print(\"Early stopping!\")\n",
    "    #     break\n",
    "    \n",
    "    # print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Epoch: {epoch} | Train loss: {train_loss:.5f} | Test loss: {test_loss:.5f}\")\n",
    "\n",
    "    f.write(str(epoch) + '\\t' + f\"{float(train_loss):.5f}\" + '\\t' + f\"{float(test_loss):.5f}\" + '\\n')\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b5e664-a346-45ab-9ab4-706a90ddf15b",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.511Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot error curves\n",
    "df_error = pd.read_csv('loss_Transformer.txt',sep='\\t', header=None, skiprows=1)\n",
    "df_error.columns=['epochs','train_loss','test_loss']\n",
    "plt.plot(df_error['train_loss'],label='train_loss')\n",
    "plt.plot(df_error['test_loss'],label='test_loss')\n",
    "plt.legend()\n",
    "plt.savefig('Transformer_loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838f12f-cce0-4ed3-95b3-5bb74cfe2cce",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.511Z"
    }
   },
   "outputs": [],
   "source": [
    "# combine batches\n",
    "test_pred = []\n",
    "with torch.inference_mode():\n",
    "    test_loss = 0\n",
    "    model.eval()\n",
    "    for X, y in test_dataloader:\n",
    "        test_pred_batch = model(X.to(device))\n",
    "        test_pred.append(test_pred_batch)\n",
    "\n",
    "test_pred = torch.cat(tuple(test_pred), dim=0)\n",
    "\n",
    "def get_original_shape(y):\n",
    "    # (space * num_window, seq_size, 124 or 128) > (space, num_window, seq_size, 124 or 128)\n",
    "    y = y.reshape(384, int(y.shape[0]/384),SEQUENCE_SIZE,128)\n",
    "    # window_1 is values in the first window, take the last value from each rest window and concat to the end.\n",
    "    # (space, num_window, seq_size, 124 or 128) > (space, time, 124 or 128)\n",
    "    window_1 = y[:, 0, :, :]\n",
    "    rest = y[:, 1:, -1, :]\n",
    "    y = torch.cat((window_1,rest),dim=1)\n",
    "    y = y.permute(1,0,2)\n",
    "    y = y.reshape(y.shape[0]*y.shape[1],128)\n",
    "    return y\n",
    "\n",
    "# get back the original shape\n",
    "y_test = get_original_shape(y_test)\n",
    "test_pred = get_original_shape(test_pred)\n",
    "\n",
    "# to cpu\n",
    "test_pred = test_pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03cb56-89e3-49b5-8214-36ced009af78",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.511Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the first sample. Delete this if not necessary.\n",
    "plt.plot(test_pred[5000,:])\n",
    "plt.plot(y_test[5000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348e211-ba79-405d-bc12-a97c33a19eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e68cf4f1-82cc-455c-b370-dc5e5c7283e6",
   "metadata": {},
   "source": [
    "# Post-processing (metric files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2cbd2b-88fa-426b-9139-a85e965d0441",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.511Z"
    }
   },
   "outputs": [],
   "source": [
    "# model name\n",
    "# (model name is used for the output)\n",
    "model_name = 'Transformer'\n",
    "# input of validation dataset (npy)\n",
    "fn_x_true = '/work/sds-lab/Shuochen/climsim/val_input.npy'\n",
    "# true output of validation dataset (npy)\n",
    "fn_y_true = '/work/sds-lab/Shuochen/climsim/val_target.npy'\n",
    "# Model predicted output of varlidation dataset (npy)\n",
    "fn_y_pred = test_pred\n",
    "# model grid information (nc)\n",
    "fn_grid = '/work/sds-lab/Shuochen/climsim/normalizations_git/ClimSim_low-res_grid-info.nc'\n",
    "\n",
    "# normalization and scale factors (nc), fn_mli files are not necessary\n",
    "fn_mli_mean  = '/work/sds-lab/Shuochen/climsim/normalizations_git/inputs/input_mean.nc'\n",
    "fn_mli_min   = '/work/sds-lab/Shuochen/climsim/normalizations_git/inputs/input_min.nc'\n",
    "fn_mli_max   = '/work/sds-lab/Shuochen/climsim/normalizations_git/inputs/input_max.nc'\n",
    "fn_mlo_scale = '/work/sds-lab/Shuochen/climsim/normalizations_git/outputs/output_scale.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1255581-b166-49ec-a1b9-17eb94313a37",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.511Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# physical constatns from (E3SM_ROOT/share/util/shr_const_mod.F90)\n",
    "grav    = 9.80616    # acceleration of gravity ~ m/s^2\n",
    "cp      = 1.00464e3  # specific heat of dry air   ~ J/kg/K\n",
    "lv      = 2.501e6    # latent heat of evaporation ~ J/kg\n",
    "lf      = 3.337e5    # latent heat of fusion      ~ J/kg\n",
    "ls      = lv + lf    # latent heat of sublimation ~ J/kg\n",
    "rho_air = 101325./ (6.02214e26*1.38065e-23/28.966) / 273.15 # density of dry air at STP  ~ kg/m^3\n",
    "                                                            # ~ 1.2923182846924677\n",
    "                                                            # SHR_CONST_PSTD/(SHR_CONST_RDAIR*SHR_CONST_TKFRZ)\n",
    "                                                            # SHR_CONST_RDAIR   = SHR_CONST_RGAS/SHR_CONST_MWDAIR\n",
    "                                                            # SHR_CONST_RGAS    = SHR_CONST_AVOGAD*SHR_CONST_BOLTZ\n",
    "rho_h20 = 1.e3       # density of fresh water     ~ kg/m^ 3\n",
    "\n",
    "vars_mlo_energy_conv = {'ptend_t':cp,\n",
    "                        'ptend_q0001':lv,\n",
    "                        'cam_out_NETSW':1.,\n",
    "                        'cam_out_FLWDS':1.,\n",
    "                        'cam_out_PRECSC':lv*rho_h20,\n",
    "                        'cam_out_PRECC':lv*rho_h20,\n",
    "                        'cam_out_SOLS':1.,\n",
    "                        'cam_out_SOLL':1.,\n",
    "                        'cam_out_SOLSD':1.,\n",
    "                        'cam_out_SOLLD':1.\n",
    "                       }\n",
    "vars_longname=\\\n",
    "{'ptend_t':'Heating tendency, ∂T/∂t',\n",
    " 'ptend_q0001':'Moistening tendency, ∂q/∂t',\n",
    " 'cam_out_NETSW':'Net surface shortwave flux, NETSW',\n",
    " 'cam_out_FLWDS':'Downward surface longwave flux, FLWDS',\n",
    " 'cam_out_PRECSC':'Snow rate, PRECSC',\n",
    " 'cam_out_PRECC':'Rain rate, PRECC',\n",
    " 'cam_out_SOLS':'Visible direct solar flux, SOLS',\n",
    " 'cam_out_SOLL':'Near-IR direct solar flux, SOLL',\n",
    " 'cam_out_SOLSD':'Visible diffused solar flux, SOLSD',\n",
    " 'cam_out_SOLLD':'Near-IR diffused solar flux, SOLLD'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a8f35-3c8f-4d94-bec2-7788b52602b8",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.511Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# set dimemsion names for xarray datasets\n",
    "dim_name_level  = 'lev'\n",
    "dim_name_sample = 'sample'\n",
    "\n",
    "# load input dataset\n",
    "x_true = np.load(fn_x_true)\n",
    "y_true = np.load(fn_y_true)\n",
    "y_pred = fn_y_pred\n",
    "N_samples = y_pred.shape[0]\n",
    "\n",
    "# load norm/scale factors\n",
    "mlo_scale = xr.open_dataset(fn_mlo_scale)\n",
    "mli_mean  = xr.open_dataset(fn_mli_mean)\n",
    "mli_min   = xr.open_dataset(fn_mli_min)\n",
    "mli_max   = xr.open_dataset(fn_mli_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fee4f-db4f-4ed6-af5d-8295475d10da",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.511Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load grid information\n",
    "ds_grid = xr.open_dataset(fn_grid) # has ncol:384\n",
    "N_ncol = len(ds_grid['ncol']) # length of ncol dimension (nlat * nlon)\n",
    "\n",
    "# make area-weights\n",
    "ds_grid['area_wgt'] = ds_grid['area'] / ds_grid['area'].mean('ncol')\n",
    "\n",
    "# map ds_grid's ncol dimension -> the N_samples dimension of npy-loayd arrays (e.g., y_pred)\n",
    "to_xarray = {'area_wgt': (dim_name_sample,np.tile(ds_grid['area_wgt'], int(N_samples/len(ds_grid['ncol'])))),\n",
    "            }\n",
    "to_xarray = xr.Dataset(to_xarray)\n",
    "\n",
    "# add nsample-mapped grid variables back to ds_grid\n",
    "ds_grid = xr.merge([ds_grid  [['P0', 'hyai', 'hyam','hybi','hybm','lat','lon','area']],\n",
    "                    to_xarray[['area_wgt']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662eb23-fd85-4edb-b2ef-636f8d2205ab",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.512Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# list of ML output variables\n",
    "vars_mlo = ['ptend_t','ptend_q0001','cam_out_NETSW','cam_out_FLWDS','cam_out_PRECSC',\n",
    "            'cam_out_PRECC','cam_out_SOLS','cam_out_SOLL','cam_out_SOLSD','cam_out_SOLLD'] # mlo mean ML output.\n",
    "\n",
    "# length of each variable\n",
    "# (make sure that the order of variables are correct)\n",
    "vars_mlo_len = {'ptend_t':60,\n",
    "                'ptend_q0001':60,\n",
    "                'cam_out_NETSW':1,\n",
    "                'cam_out_FLWDS':1,\n",
    "                'cam_out_PRECSC':1,\n",
    "                'cam_out_PRECC':1,\n",
    "                'cam_out_SOLS':1,\n",
    "                'cam_out_SOLL':1,\n",
    "                'cam_out_SOLSD':1,\n",
    "                'cam_out_SOLLD':1\n",
    "               }\n",
    "\n",
    "# map the length of dimension to the name of dimension\n",
    "len_to_dim = {60:dim_name_level,\n",
    "              N_samples: dim_name_sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95076025-91d6-4ea3-96c3-e875887b079c",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.512Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Here, we first construct a dictionary of {var name: (dimension name, array-like)},\n",
    "# then, map the dictionary to an Xarray Dataset.\n",
    "# (ref: https://docs.xarray.dev/en/stable/generated/xarray.Dataset.html)\n",
    "\n",
    "DS = {}\n",
    "\n",
    "for kds in ['true', 'pred']:\n",
    "    if kds=='true':\n",
    "        work = y_true\n",
    "    elif kds=='pred':\n",
    "        work = y_pred\n",
    "\n",
    "    # [1] Construct dictionary for xarray dataset\n",
    "    #     format is key for variable name /\n",
    "    #               value for a turple of (dimension names, data).\n",
    "    to_xarray = {}\n",
    "    for k, kvar in enumerate(vars_mlo):\n",
    "\n",
    "        # length of variable (ie, number of levels)\n",
    "        kvar_len = vars_mlo_len[kvar]\n",
    "\n",
    "        # set dimensions of variable\n",
    "        if kvar_len == 60:\n",
    "            kvar_dims = (dim_name_sample, dim_name_level)\n",
    "        elif kvar_len == 1:\n",
    "            kvar_dims = dim_name_sample\n",
    "\n",
    "        # set start and end indices of variable in the loaded numpy array\n",
    "        # then, add 'kvar':(kvar_dims, <np_array>) to dictionary\n",
    "        if k==0: ind1=0\n",
    "        ind2 = ind1 + kvar_len\n",
    "\n",
    "        # scaled output\n",
    "        kvar_data = np.squeeze(work[:,ind1:ind2])\n",
    "        # unscaled output\n",
    "        kvar_data = kvar_data / mlo_scale[kvar].values\n",
    "\n",
    "        to_xarray[kvar] = (kvar_dims, kvar_data)\n",
    "\n",
    "        ind1 = ind2\n",
    "\n",
    "    # [2] convert dict to xarray dataset\n",
    "    DS[kds] = xr.Dataset(to_xarray)\n",
    "\n",
    "    # [3] add surface pressure ('state_ps') from ml input\n",
    "    # normalized ps\n",
    "    state_ps =  xr.DataArray(x_true[:,120], dims=('sample'), name='state_ps')\n",
    "    # denormalized ps\n",
    "    state_ps = state_ps * (mli_max['state_ps'] - mli_min['state_ps']) + mli_mean['state_ps']\n",
    "    DS[kds]['state_ps'] = state_ps\n",
    "\n",
    "    # [4] add grid information\n",
    "    DS[kds] = xr.merge([DS[kds], ds_grid])\n",
    "\n",
    "    # [5] add pressure thickness of each level, dp\n",
    "    # FYI, in a hybrid sigma vertical coordinate system, pressure at level z is\n",
    "    # P[x,z] = hyam[z]*P0 + hybm[z]*PS[x,z],\n",
    "    # where, hyam and hybm are \n",
    "    tmp = DS[kds]['P0']*DS[kds]['hyai'] + DS[kds]['state_ps']*DS[kds]['hybi']\n",
    "    tmp = tmp.isel(ilev=slice(1,61)).values - tmp.isel(ilev=slice(0,60)).values\n",
    "    tmp = tmp.transpose()\n",
    "    DS[kds]['dp'] = xr.DataArray(tmp, dims=('sample', 'lev'))\n",
    "\n",
    "    # [6] break (sample) to (ncol,time)\n",
    "    N_timestep = int(N_samples/N_ncol)\n",
    "    dim_ncol     = np.arange(N_ncol)\n",
    "    dim_timestep = np.arange(N_timestep)\n",
    "    new_ind = pd.MultiIndex.from_product([dim_timestep, dim_ncol],\n",
    "                                         names=['time', 'ncol'])\n",
    "    DS[kds] = DS[kds].assign_coords(sample=new_ind).unstack('sample')\n",
    "\n",
    "del work, to_xarray, y_true, y_pred, x_true, state_ps, tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f8938-94f4-4331-ae4a-49ba3d3c7f9c",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.512Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# [1] Weight vertical levels by dp/g that is equivalent to a mass of air within a grid cell per unit area [kg/m2]\n",
    "# [2] Weight horizontal area of each grid cell by a[x]/mean(a[x]).\n",
    "# [3] Unit conversion to a common energy unit\n",
    "\n",
    "DS_ENERGY = {}\n",
    "for kds in ['true','pred']:\n",
    "    # Make a copy to keep original dataset\n",
    "    DS_ENERGY[kds] = DS[kds].copy(deep=True)\n",
    "\n",
    "    # vertical weighting / area weighting / unit conversion\n",
    "    for kvar in vars_mlo:\n",
    "\n",
    "        # [1] weight vertical levels by dp/g\n",
    "        #     ONLY for vertically-resolved variables, e.g., ptend_{t,q0001}\n",
    "        # dp/g = - \\rho * dz\n",
    "        if vars_mlo_len[kvar] == 60:\n",
    "            DS_ENERGY[kds][kvar] = DS_ENERGY[kds][kvar] * DS_ENERGY[kds]['dp']/grav\n",
    "\n",
    "        # [2] weight area\n",
    "        #     for ALL variables\n",
    "        DS_ENERGY[kds][kvar] = DS_ENERGY[kds]['area_wgt'] * DS_ENERGY[kds][kvar]\n",
    "\n",
    "        # [3] convert units to W/m2\n",
    "        #     for variables with different units, e.g., ptend_{t,q0001}, precsc, precc\n",
    "        DS_ENERGY[kds][kvar] =  vars_mlo_energy_conv[kvar] * DS_ENERGY[kds][kvar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e6771f-65d1-4f48-b89e-1a04b73e9c0c",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.512Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_metrics = ['MAE','RMSE','R2']\n",
    "\n",
    "# A. Calculate metrics\n",
    "# After this step,\n",
    "# ptend_{t,q0001} have [ncol, lev] dimension;\n",
    "# and the rest variables have [ncol] dimension.\n",
    "\n",
    "# if spatial analysis is desired (e.g., R2 distribution on global map or on latitude-level plane),\n",
    "# the metrics at this step should be used.\n",
    "\n",
    "\n",
    "# Select only ML output varibles\n",
    "DS_ENERGY[kds] = DS_ENERGY[kds][vars_mlo]\n",
    "\n",
    "# Caclulate 3 metrics\n",
    "Metrics = {}\n",
    "Metrics['MAE']  = (np.abs(DS_ENERGY['true']   - DS_ENERGY['pred'])).mean('time')\n",
    "Metrics['RMSE'] = np.sqrt(((DS_ENERGY['true'] - DS_ENERGY['pred'])**2.).mean('time'))\n",
    "Metrics['R2'] = 1 - ((DS_ENERGY['true'] - DS_ENERGY['pred']                    )**2.).sum('time')/\\\n",
    "                    ((DS_ENERGY['true'] - DS_ENERGY['true'].mean('time'))**2.).sum('time')\n",
    "\n",
    "# Save grid-wise metric files in netcdf format\n",
    "if True:\n",
    "    for kmetric in ['MAE', 'RMSE', 'R2']:\n",
    "        fn_save = f'./metrics_netcdf/{model_name}_{kmetric}.nc'\n",
    "        Metrics[kmetric].to_netcdf(fn_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152d5bd-1ee7-437a-9651-2bca891eeef8",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.513Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# B. Make horizontal mean.\n",
    "# After this step,\n",
    "# ptend_{t,q0001} have [lev] dimension;\n",
    "# and the rest variables have zero dimensions, i.e., scalars.\n",
    "\n",
    "for kmetric in all_metrics:\n",
    "    Metrics[kmetric] = Metrics[kmetric].mean('ncol') # simple mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b06bf1-8300-432f-846b-afdb157b62e3",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.513Z"
    }
   },
   "outputs": [],
   "source": [
    "# C-1. Save the result after B.\n",
    "# to save in a table format as a csv file, the level dimensions are flattened.\n",
    "\n",
    "Metrics_stacked = {}\n",
    "for kmetric in all_metrics:\n",
    "    Metrics_stacked[kmetric] = Metrics[kmetric].to_stacked_array('ml_out_idx', sample_dims='', name=kmetric)\n",
    "\n",
    "\n",
    "# save the output\n",
    "work = pd.DataFrame({'MAE':  Metrics_stacked['MAE'].values,\n",
    "                     'RMSE': Metrics_stacked['RMSE'].values,\n",
    "                     'R2':   Metrics_stacked['R2'].values}\n",
    "                    )\n",
    "work.index.name = 'output_idx'\n",
    "\n",
    "# save to the metrics folder\n",
    "fn_save_metrics = f'./metrics/{model_name}.metrics.csv'\n",
    "work.to_csv(fn_save_metrics)\n",
    "work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908a200-7e84-4335-9097-165071a774a4",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.513Z"
    }
   },
   "outputs": [],
   "source": [
    "# try to reproduce their plot 2\n",
    "work['MAE'][:60].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788abb9-b0db-49ef-b7db-3ff0012d719a",
   "metadata": {
    "execution": {
     "execution_failed": "2024-07-22T04:00:25.513Z"
    }
   },
   "outputs": [],
   "source": [
    "# C-2. Save the result after vertical averaging.\n",
    "# After this step,\n",
    "# ptend_{t,q0001} also have zero dimensions, i.e., scalars;\n",
    "\n",
    "# Then, the results are saved to a csv file.\n",
    "# This csv file will be used for generating plots.\n",
    "\n",
    "Metrics_vert_avg = {}\n",
    "for kmetric in all_metrics:\n",
    "    Metrics_vert_avg[kmetric] = Metrics[kmetric].mean('lev')\n",
    "    Metrics_vert_avg[kmetric] = Metrics_vert_avg[kmetric].mean('ilev') # remove dummy dim\n",
    "\n",
    "# save the output\n",
    "work = pd.DataFrame({'MAE':  Metrics_vert_avg['MAE'].to_pandas(),\n",
    "                     'RMSE': Metrics_vert_avg['RMSE'].to_pandas(),\n",
    "                     'R2':   Metrics_vert_avg['R2'].to_pandas()}\n",
    "                    )\n",
    "work.index.name = 'Variable'\n",
    "\n",
    "# save to the metrics folder\n",
    "fn_save_metrics_avg = f'./metrics/{model_name}.metrics.lev-avg.csv'\n",
    "work.to_csv(fn_save_metrics_avg)\n",
    "work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1c817-83c0-4450-9a2c-d97390614cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
