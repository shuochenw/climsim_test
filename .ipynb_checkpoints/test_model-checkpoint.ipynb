{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9953c2-428d-4a1b-82de-1ef321af3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c9288-c0fd-4318-9ab5-61fb6dd1f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a1c38-e583-4919-a411-760ad7e8e829",
   "metadata": {},
   "source": [
    "# load datasets: use 1 year or 8 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c7c09-88b5-40f0-9073-53bd332c0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "IN_FEATURES = 124\n",
    "OUT_FEATURES = 128\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57475c92-db79-4854-983d-36e58b80601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use validation year: change path\n",
    "X=np.load('/work/sds-lab/Shuochen/climsim/val_input.npy')\n",
    "y=np.load('/work/sds-lab/Shuochen/climsim/val_target.npy')\n",
    "\n",
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# # use all 8 years: change path\n",
    "# X_train=np.load('/work/sds-lab/Shuochen/climsim/train_input.npy')\n",
    "# y_train=np.load('/work/sds-lab/Shuochen/climsim/train_target.npy')\n",
    "# X_test=np.load('/work/sds-lab/Shuochen/climsim/val_input.npy')\n",
    "# y_test=np.load('/work/sds-lab/Shuochen/climsim/val_target.npy')\n",
    "\n",
    "# X_train = torch.from_numpy(X_train).type(torch.float)\n",
    "# y_train = torch.from_numpy(y_train).type(torch.float)\n",
    "# X_test = torch.from_numpy(X_test).type(torch.float)\n",
    "# y_test = torch.from_numpy(y_test).type(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5297d8a2-75ae-48a6-a1c1-5982b018af28",
   "metadata": {},
   "source": [
    "# train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe73eae-9aa9-4a1d-b61e-30b69f8749c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split datasets to training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "# create datasets\n",
    "training_set = TensorDataset(X_train, y_train)\n",
    "testing_set = TensorDataset(X_test, y_test)\n",
    "# create dataloaders\n",
    "train_dataloader = DataLoader(training_set, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "test_dataloader = DataLoader(testing_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3774d4e2-1950-4a7b-84f4-8e93a1a3de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    def __init__(self, IN_FEATURES, hidden_dim, OUT_FEATURES):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(IN_FEATURES, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, OUT_FEATURES)\n",
    "    def forward(self,x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n",
    "        \n",
    "model = FCNN(IN_FEATURES, 64, OUT_FEATURES).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda8d81-218d-424b-ab2a-302aabf4ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        y_pred = model(X.to(device))\n",
    "        loss = loss_fn(y_pred, y.to(device))\n",
    "        train_loss += loss # accumulatively add up the loss per epoch \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        test_loss = 0\n",
    "        model.eval()\n",
    "        for X, y in test_dataloader:\n",
    "            test_pred = model(X.to(device))\n",
    "            test_loss += loss_fn(test_pred, y.to(device)) # accumulatively add up the loss per epoch\n",
    "        \n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "    print(f\"Epoch: {epoch} | Train loss: {train_loss:.5f} | Test loss: {test_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c564d-9d6d-4bae-8310-6bbae59f2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine batches\n",
    "model = model.to('cpu')\n",
    "test_pred = model(X_test)\n",
    "test_pred = test_pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e35f16-4391-4a3d-9ee8-077a12f096bf",
   "metadata": {},
   "source": [
    "# plot prediction and testing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cba5e1-e5af-4f03-93f9-bcea06da910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_pred[0,:])\n",
    "plt.plot(y_test[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6f3e2-931f-4cb4-a07f-68cc0fb9043e",
   "metadata": {},
   "source": [
    "# Post-processing (generate metric files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964474e9-427d-435a-b8fa-3a75297b35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change model name\n",
    "model_name = 'FCNN'\n",
    "# input of validation dataset (npy)\n",
    "fn_x_true = '/work/sds-lab/Shuochen/climsim/val_input.npy'\n",
    "# true output of validation dataset (npy)\n",
    "fn_y_true = '/work/sds-lab/Shuochen/climsim/val_target.npy'\n",
    "# Model predicted output of varlidation dataset (npy)\n",
    "fn_y_pred = test_pred\n",
    "# model grid information (nc)\n",
    "fn_grid = '/work/sds-lab/Shuochen/climsim/normalizations_git/ClimSim_low-res_grid-info.nc'\n",
    "\n",
    "# normalization and scale factors (nc)\n",
    "fn_mli_mean  = '/work/sds-lab/Shuochen/climsim/normalizations_git/inputs/input_mean.nc'\n",
    "fn_mli_min   = '/work/sds-lab/Shuochen/climsim/normalizations_git/inputs/input_min.nc'\n",
    "fn_mli_max   = '/work/sds-lab/Shuochen/climsim/normalizations_git/inputs/input_max.nc'\n",
    "fn_mlo_scale = '/work/sds-lab/Shuochen/climsim/normalizations_git/outputs/output_scale.nc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8822b-3218-4465-a277-d76b67f59096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical constatns from (E3SM_ROOT/share/util/shr_const_mod.F90)\n",
    "grav    = 9.80616    # acceleration of gravity ~ m/s^2\n",
    "cp      = 1.00464e3  # specific heat of dry air   ~ J/kg/K\n",
    "lv      = 2.501e6    # latent heat of evaporation ~ J/kg\n",
    "lf      = 3.337e5    # latent heat of fusion      ~ J/kg\n",
    "ls      = lv + lf    # latent heat of sublimation ~ J/kg\n",
    "rho_air = 101325./ (6.02214e26*1.38065e-23/28.966) / 273.15 # density of dry air at STP  ~ kg/m^3\n",
    "                                                            # ~ 1.2923182846924677\n",
    "                                                            # SHR_CONST_PSTD/(SHR_CONST_RDAIR*SHR_CONST_TKFRZ)\n",
    "                                                            # SHR_CONST_RDAIR   = SHR_CONST_RGAS/SHR_CONST_MWDAIR\n",
    "                                                            # SHR_CONST_RGAS    = SHR_CONST_AVOGAD*SHR_CONST_BOLTZ\n",
    "rho_h20 = 1.e3       # density of fresh water     ~ kg/m^ 3\n",
    "\n",
    "vars_mlo_energy_conv = {'ptend_t':cp,\n",
    "                        'ptend_q0001':lv,\n",
    "                        'cam_out_NETSW':1.,\n",
    "                        'cam_out_FLWDS':1.,\n",
    "                        'cam_out_PRECSC':lv*rho_h20,\n",
    "                        'cam_out_PRECC':lv*rho_h20,\n",
    "                        'cam_out_SOLS':1.,\n",
    "                        'cam_out_SOLL':1.,\n",
    "                        'cam_out_SOLSD':1.,\n",
    "                        'cam_out_SOLLD':1.\n",
    "                       }\n",
    "vars_longname=\\\n",
    "{'ptend_t':'Heating tendency, ∂T/∂t',\n",
    " 'ptend_q0001':'Moistening tendency, ∂q/∂t',\n",
    " 'cam_out_NETSW':'Net surface shortwave flux, NETSW',\n",
    " 'cam_out_FLWDS':'Downward surface longwave flux, FLWDS',\n",
    " 'cam_out_PRECSC':'Snow rate, PRECSC',\n",
    " 'cam_out_PRECC':'Rain rate, PRECC',\n",
    " 'cam_out_SOLS':'Visible direct solar flux, SOLS',\n",
    " 'cam_out_SOLL':'Near-IR direct solar flux, SOLL',\n",
    " 'cam_out_SOLSD':'Visible diffused solar flux, SOLSD',\n",
    " 'cam_out_SOLLD':'Near-IR diffused solar flux, SOLLD'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f76525-bca1-44bf-a2a6-bd135822e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dimemsion names for xarray datasets\n",
    "dim_name_level  = 'lev'\n",
    "dim_name_sample = 'sample'\n",
    "\n",
    "# load input dataset\n",
    "x_true = np.load(fn_x_true)\n",
    "y_true = np.load(fn_y_true)\n",
    "y_pred = fn_y_pred\n",
    "N_samples = y_pred.shape[0]\n",
    "\n",
    "# load norm/scale factors\n",
    "mlo_scale = xr.open_dataset(fn_mlo_scale)\n",
    "mli_mean  = xr.open_dataset(fn_mli_mean)\n",
    "mli_min   = xr.open_dataset(fn_mli_min)\n",
    "mli_max   = xr.open_dataset(fn_mli_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79257449-762c-4a90-8df4-e5b740e8acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load grid information\n",
    "ds_grid = xr.open_dataset(fn_grid) # has ncol:384\n",
    "N_ncol = len(ds_grid['ncol']) # length of ncol dimension (nlat * nlon)\n",
    "\n",
    "# make area-weights\n",
    "ds_grid['area_wgt'] = ds_grid['area'] / ds_grid['area'].mean('ncol')\n",
    "\n",
    "# map ds_grid's ncol dimension -> the N_samples dimension of npy-loayd arrays (e.g., y_pred)\n",
    "to_xarray = {'area_wgt': (dim_name_sample,np.tile(ds_grid['area_wgt'], int(N_samples/len(ds_grid['ncol'])))),\n",
    "            }\n",
    "to_xarray = xr.Dataset(to_xarray)\n",
    "\n",
    "# add nsample-mapped grid variables back to ds_grid\n",
    "ds_grid = xr.merge([ds_grid  [['P0', 'hyai', 'hyam','hybi','hybm','lat','lon','area']],\n",
    "                    to_xarray[['area_wgt']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54989c71-cc98-4b59-8837-e1473769181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of ML output variables\n",
    "vars_mlo = ['ptend_t','ptend_q0001','cam_out_NETSW','cam_out_FLWDS','cam_out_PRECSC',\n",
    "            'cam_out_PRECC','cam_out_SOLS','cam_out_SOLL','cam_out_SOLSD','cam_out_SOLLD'] # mlo mean ML output.\n",
    "\n",
    "# length of each variable\n",
    "# (make sure that the order of variables are correct)\n",
    "vars_mlo_len = {'ptend_t':60,\n",
    "                'ptend_q0001':60,\n",
    "                'cam_out_NETSW':1,\n",
    "                'cam_out_FLWDS':1,\n",
    "                'cam_out_PRECSC':1,\n",
    "                'cam_out_PRECC':1,\n",
    "                'cam_out_SOLS':1,\n",
    "                'cam_out_SOLL':1,\n",
    "                'cam_out_SOLSD':1,\n",
    "                'cam_out_SOLLD':1\n",
    "               }\n",
    "\n",
    "# map the length of dimension to the name of dimension\n",
    "len_to_dim = {60:dim_name_level,\n",
    "              N_samples: dim_name_sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a6fb1-049d-4440-8748-f4c81f172e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we first construct a dictionary of {var name: (dimension name, array-like)},\n",
    "# then, map the dictionary to an Xarray Dataset.\n",
    "# (ref: https://docs.xarray.dev/en/stable/generated/xarray.Dataset.html)\n",
    "\n",
    "DS = {}\n",
    "\n",
    "for kds in ['true', 'pred']:\n",
    "    if kds=='true':\n",
    "        work = y_true\n",
    "    elif kds=='pred':\n",
    "        work = y_pred\n",
    "\n",
    "    # [1] Construct dictionary for xarray dataset\n",
    "    #     format is key for variable name /\n",
    "    #               value for a turple of (dimension names, data).\n",
    "    to_xarray = {}\n",
    "    for k, kvar in enumerate(vars_mlo):\n",
    "\n",
    "        # length of variable (ie, number of levels)\n",
    "        kvar_len = vars_mlo_len[kvar]\n",
    "\n",
    "        # set dimensions of variable\n",
    "        if kvar_len == 60:\n",
    "            kvar_dims = (dim_name_sample, dim_name_level)\n",
    "        elif kvar_len == 1:\n",
    "            kvar_dims = dim_name_sample\n",
    "\n",
    "        # set start and end indices of variable in the loaded numpy array\n",
    "        # then, add 'kvar':(kvar_dims, <np_array>) to dictionary\n",
    "        if k==0: ind1=0\n",
    "        ind2 = ind1 + kvar_len\n",
    "\n",
    "        # scaled output\n",
    "        kvar_data = np.squeeze(work[:,ind1:ind2])\n",
    "        # unscaled output\n",
    "        kvar_data = kvar_data / mlo_scale[kvar].values\n",
    "\n",
    "        to_xarray[kvar] = (kvar_dims, kvar_data)\n",
    "\n",
    "        ind1 = ind2\n",
    "\n",
    "    # [2] convert dict to xarray dataset\n",
    "    DS[kds] = xr.Dataset(to_xarray)\n",
    "\n",
    "    # [3] add surface pressure ('state_ps') from ml input\n",
    "    # normalized ps\n",
    "    state_ps =  xr.DataArray(x_true[:,120], dims=('sample'), name='state_ps')\n",
    "    # denormalized ps\n",
    "    state_ps = state_ps * (mli_max['state_ps'] - mli_min['state_ps']) + mli_mean['state_ps']\n",
    "    DS[kds]['state_ps'] = state_ps\n",
    "\n",
    "    # [4] add grid information\n",
    "    DS[kds] = xr.merge([DS[kds], ds_grid])\n",
    "\n",
    "    # [5] add pressure thickness of each level, dp\n",
    "    # FYI, in a hybrid sigma vertical coordinate system, pressure at level z is\n",
    "    # P[x,z] = hyam[z]*P0 + hybm[z]*PS[x,z],\n",
    "    # where, hyam and hybm are \n",
    "    tmp = DS[kds]['P0']*DS[kds]['hyai'] + DS[kds]['state_ps']*DS[kds]['hybi']\n",
    "    tmp = tmp.isel(ilev=slice(1,61)).values - tmp.isel(ilev=slice(0,60)).values\n",
    "    tmp = tmp.transpose()\n",
    "    DS[kds]['dp'] = xr.DataArray(tmp, dims=('sample', 'lev'))\n",
    "\n",
    "    # [6] break (sample) to (ncol,time)\n",
    "    N_timestep = int(N_samples/N_ncol)\n",
    "    dim_ncol     = np.arange(N_ncol)\n",
    "    dim_timestep = np.arange(N_timestep)\n",
    "    new_ind = pd.MultiIndex.from_product([dim_timestep, dim_ncol],\n",
    "                                         names=['time', 'ncol'])\n",
    "    DS[kds] = DS[kds].assign_coords(sample=new_ind).unstack('sample')\n",
    "\n",
    "del work, to_xarray, y_true, y_pred, x_true, state_ps, tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9db0fb-f086-44c6-a438-d1b9971c2179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] Weight vertical levels by dp/g that is equivalent to a mass of air within a grid cell per unit area [kg/m2]\n",
    "# [2] Weight horizontal area of each grid cell by a[x]/mean(a[x]).\n",
    "# [3] Unit conversion to a common energy unit\n",
    "\n",
    "DS_ENERGY = {}\n",
    "for kds in ['true','pred']:\n",
    "    # Make a copy to keep original dataset\n",
    "    DS_ENERGY[kds] = DS[kds].copy(deep=True)\n",
    "\n",
    "    # vertical weighting / area weighting / unit conversion\n",
    "    for kvar in vars_mlo:\n",
    "\n",
    "        # [1] weight vertical levels by dp/g\n",
    "        #     ONLY for vertically-resolved variables, e.g., ptend_{t,q0001}\n",
    "        # dp/g = - \\rho * dz\n",
    "        if vars_mlo_len[kvar] == 60:\n",
    "            DS_ENERGY[kds][kvar] = DS_ENERGY[kds][kvar] * DS_ENERGY[kds]['dp']/grav\n",
    "\n",
    "        # [2] weight area\n",
    "        #     for ALL variables\n",
    "        DS_ENERGY[kds][kvar] = DS_ENERGY[kds]['area_wgt'] * DS_ENERGY[kds][kvar]\n",
    "\n",
    "        # [3] convert units to W/m2\n",
    "        #     for variables with different units, e.g., ptend_{t,q0001}, precsc, precc\n",
    "        DS_ENERGY[kds][kvar] =  vars_mlo_energy_conv[kvar] * DS_ENERGY[kds][kvar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa1aad-8b82-4fc8-afce-6bd05c188387",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = ['MAE','RMSE','R2']\n",
    "\n",
    "# A. Calculate metrics\n",
    "# After this step,\n",
    "# ptend_{t,q0001} have [ncol, lev] dimension;\n",
    "# and the rest variables have [ncol] dimension.\n",
    "\n",
    "# if spatial analysis is desired (e.g., R2 distribution on global map or on latitude-level plane),\n",
    "# the metrics at this step should be used.\n",
    "\n",
    "\n",
    "# Select only ML output varibles\n",
    "DS_ENERGY[kds] = DS_ENERGY[kds][vars_mlo]\n",
    "\n",
    "# Caclulate 3 metrics\n",
    "Metrics = {}\n",
    "Metrics['MAE']  = (np.abs(DS_ENERGY['true']   - DS_ENERGY['pred'])).mean('time')\n",
    "Metrics['RMSE'] = np.sqrt(((DS_ENERGY['true'] - DS_ENERGY['pred'])**2.).mean('time'))\n",
    "Metrics['R2'] = 1 - ((DS_ENERGY['true'] - DS_ENERGY['pred']                    )**2.).sum('time')/\\\n",
    "                    ((DS_ENERGY['true'] - DS_ENERGY['true'].mean('time'))**2.).sum('time')\n",
    "\n",
    "# Save grid-wise metric files in netcdf format\n",
    "if True:\n",
    "    for kmetric in ['MAE', 'RMSE', 'R2']:\n",
    "        fn_save = f'./metrics_netcdf/{model_name}_{kmetric}.nc'\n",
    "        Metrics[kmetric].to_netcdf(fn_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113ef51f-a97e-4c65-976a-d86cb2de371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Make horizontal mean.\n",
    "# After this step,\n",
    "# ptend_{t,q0001} have [lev] dimension;\n",
    "# and the rest variables have zero dimensions, i.e., scalars.\n",
    "\n",
    "for kmetric in all_metrics:\n",
    "    Metrics[kmetric] = Metrics[kmetric].mean('ncol') # simple mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e64b4-0d50-4c98-bb06-9643313b94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-1. Save the result after B.\n",
    "# to save in a table format as a csv file, the level dimensions are flattened.\n",
    "\n",
    "Metrics_stacked = {}\n",
    "for kmetric in all_metrics:\n",
    "    Metrics_stacked[kmetric] = Metrics[kmetric].to_stacked_array('ml_out_idx', sample_dims='', name=kmetric)\n",
    "\n",
    "\n",
    "# save the output\n",
    "work = pd.DataFrame({'MAE':  Metrics_stacked['MAE'].values,\n",
    "                     'RMSE': Metrics_stacked['RMSE'].values,\n",
    "                     'R2':   Metrics_stacked['R2'].values}\n",
    "                    )\n",
    "work.index.name = 'output_idx'\n",
    "\n",
    "# save to the metrics folder\n",
    "fn_save_metrics = f'./metrics/{model_name}.metrics.csv'\n",
    "work.to_csv(fn_save_metrics)\n",
    "work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360e249a-6c4a-4a42-b85e-0e9cc6319ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-2. Save the result after vertical averaging.\n",
    "# After this step,\n",
    "# ptend_{t,q0001} also have zero dimensions, i.e., scalars;\n",
    "\n",
    "# Then, the results are saved to a csv file.\n",
    "# This csv file will be used for generating plots.\n",
    "\n",
    "Metrics_vert_avg = {}\n",
    "for kmetric in all_metrics:\n",
    "    Metrics_vert_avg[kmetric] = Metrics[kmetric].mean('lev')\n",
    "    Metrics_vert_avg[kmetric] = Metrics_vert_avg[kmetric].mean('ilev') # remove dummy dim\n",
    "\n",
    "# save the output\n",
    "work = pd.DataFrame({'MAE':  Metrics_vert_avg['MAE'].to_pandas(),\n",
    "                     'RMSE': Metrics_vert_avg['RMSE'].to_pandas(),\n",
    "                     'R2':   Metrics_vert_avg['R2'].to_pandas()}\n",
    "                    )\n",
    "work.index.name = 'Variable'\n",
    "\n",
    "# save to the metrics folder\n",
    "fn_save_metrics_avg = f'./metrics/{model_name}.metrics.lev-avg.csv'\n",
    "work.to_csv(fn_save_metrics_avg)\n",
    "work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cecf2ad-cc3c-404a-b372-4f1e483f0c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd569c76-5426-4f96-863d-2c377744d354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
