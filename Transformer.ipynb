{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c3c21b-23f5-45f2-9460-4ee642157802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T16:55:11.583769Z",
     "iopub.status.busy": "2024-03-02T16:55:11.583556Z",
     "iopub.status.idle": "2024-03-02T16:55:15.564306Z",
     "shell.execute_reply": "2024-03-02T16:55:15.563886Z",
     "shell.execute_reply.started": "2024-03-02T16:55:11.583757Z"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1e9a38-9011-4f18-b213-b1f8abf8179f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T16:55:15.565032Z",
     "iopub.status.busy": "2024-03-02T16:55:15.564828Z",
     "iopub.status.idle": "2024-03-02T16:55:15.595446Z",
     "shell.execute_reply": "2024-03-02T16:55:15.595162Z",
     "shell.execute_reply.started": "2024-03-02T16:55:15.565020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298e3ed2-a774-46ed-9280-ad057695527d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T16:55:15.596408Z",
     "iopub.status.busy": "2024-03-02T16:55:15.596208Z",
     "iopub.status.idle": "2024-03-02T16:55:16.674244Z",
     "shell.execute_reply": "2024-03-02T16:55:16.673769Z",
     "shell.execute_reply.started": "2024-03-02T16:55:15.596397Z"
    }
   },
   "outputs": [],
   "source": [
    "X=np.load('/work/sds-lab/Shuochen/climsim/val_input.npy')\n",
    "y=np.load('/work/sds-lab/Shuochen/climsim/val_target.npy')\n",
    "\n",
    "X = torch.from_numpy(X).type(torch.float).to(device)\n",
    "y = torch.from_numpy(y).type(torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d23a4e-d11f-4d02-bd0a-5a852a0e1b6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T16:55:16.674968Z",
     "iopub.status.busy": "2024-03-02T16:55:16.674840Z",
     "iopub.status.idle": "2024-03-02T16:55:16.723672Z",
     "shell.execute_reply": "2024-03-02T16:55:16.723353Z",
     "shell.execute_reply.started": "2024-03-02T16:55:16.674955Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "IN_FEATURES = 124\n",
    "OUT_FEATURES = 128\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ecf76b2-6bb6-4464-a89e-80683e0d135c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T16:55:16.724260Z",
     "iopub.status.busy": "2024-03-02T16:55:16.724130Z",
     "iopub.status.idle": "2024-03-02T16:55:16.727004Z",
     "shell.execute_reply": "2024-03-02T16:55:16.726742Z",
     "shell.execute_reply.started": "2024-03-02T16:55:16.724249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup data loaders for batch\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d36a4288-fdb2-4ca4-b554-55baa8896197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T16:55:16.727650Z",
     "iopub.status.busy": "2024-03-02T16:55:16.727389Z",
     "iopub.status.idle": "2024-03-02T16:55:16.748281Z",
     "shell.execute_reply": "2024-03-02T16:55:16.748014Z",
     "shell.execute_reply.started": "2024-03-02T16:55:16.727639Z"
    }
   },
   "outputs": [],
   "source": [
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bee7c252-84a3-4bf5-8795-8cb40d91e834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T16:55:16.748786Z",
     "iopub.status.busy": "2024-03-02T16:55:16.748670Z",
     "iopub.status.idle": "2024-03-02T16:55:16.788431Z",
     "shell.execute_reply": "2024-03-02T16:55:16.788153Z",
     "shell.execute_reply.started": "2024-03-02T16:55:16.748776Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wang.shuoc/.conda/envs/my_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Model definition using Transformer\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim=IN_FEATURES, output_dim = OUT_FEATURES, d_model=64, nhead=4, num_layers=2, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, OUT_FEATURES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.decoder(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "model = TransformerModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ebc289c-e381-433b-80b5-49be97083323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T16:55:16.790269Z",
     "iopub.status.busy": "2024-03-02T16:55:16.789456Z",
     "iopub.status.idle": "2024-03-02T20:45:47.234925Z",
     "shell.execute_reply": "2024-03-02T20:45:47.234413Z",
     "shell.execute_reply.started": "2024-03-02T16:55:16.790256Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wang.shuoc/.conda/envs/my_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Test loss: 0.01355045661330223\n",
      "Epoch: 1 Test loss: 0.013412756845355034\n",
      "Epoch: 2 Test loss: 0.013395403511822224\n",
      "Epoch: 3 Test loss: 0.013490053825080395\n",
      "Epoch: 4 Test loss: 0.013409774750471115\n",
      "Epoch: 5 Test loss: 0.01336912252008915\n",
      "Epoch: 6 Test loss: 0.013655360788106918\n",
      "Epoch: 7 Test loss: 0.013361834920942783\n",
      "Epoch: 8 Test loss: 0.013327253051102161\n",
      "Epoch: 9 Test loss: 0.013392176479101181\n",
      "Epoch: 10 Test loss: 0.01334233395755291\n",
      "Epoch: 11 Test loss: 0.013339066877961159\n",
      "Epoch: 12 Test loss: 0.0133614931255579\n",
      "Epoch: 13 Test loss: 0.013331624679267406\n",
      "Epoch: 14 Test loss: 0.013303442858159542\n",
      "Epoch: 15 Test loss: 0.013338404707610607\n",
      "Epoch: 16 Test loss: 0.013479919172823429\n",
      "Epoch: 17 Test loss: 0.013285274617373943\n",
      "Epoch: 18 Test loss: 0.01331124734133482\n",
      "Epoch: 19 Test loss: 0.013345104642212391\n",
      "Epoch: 20 Test loss: 0.013304116204380989\n",
      "Epoch: 21 Test loss: 0.013317163102328777\n",
      "Epoch: 22 Test loss: 0.013308174908161163\n",
      "Epoch: 23 Test loss: 0.01333095133304596\n",
      "Epoch: 24 Test loss: 0.013314398005604744\n",
      "Epoch: 25 Test loss: 0.013326933607459068\n",
      "Epoch: 26 Test loss: 0.013320577330887318\n",
      "Epoch: 27 Test loss: 0.013302706182003021\n",
      "Epoch: 28 Test loss: 0.013316364958882332\n",
      "Epoch: 29 Test loss: 0.013319832272827625\n",
      "Epoch: 30 Test loss: 0.013316543772816658\n",
      "Epoch: 31 Test loss: 0.013307704590260983\n",
      "Epoch: 32 Test loss: 0.013314437121152878\n",
      "Epoch: 33 Test loss: 0.013307441025972366\n",
      "Epoch: 34 Test loss: 0.013203339651226997\n",
      "Epoch: 35 Test loss: 0.013203875161707401\n",
      "Epoch: 36 Test loss: 0.01322085876017809\n",
      "Epoch: 37 Test loss: 0.013232206925749779\n",
      "Epoch: 38 Test loss: 0.01321010198444128\n",
      "Epoch: 39 Test loss: 0.013198461383581161\n",
      "Epoch: 40 Test loss: 0.013211183249950409\n",
      "Epoch: 41 Test loss: 0.013202650472521782\n",
      "Epoch: 42 Test loss: 0.013096610084176064\n",
      "Epoch: 43 Test loss: 0.013133009895682335\n",
      "Epoch: 44 Test loss: 0.01309660729020834\n",
      "Epoch: 45 Test loss: 0.013108361512422562\n",
      "Epoch: 46 Test loss: 0.01311550848186016\n",
      "Epoch: 47 Test loss: 0.013131081126630306\n",
      "Epoch: 48 Test loss: 0.013110686093568802\n",
      "Epoch: 49 Test loss: 0.01308702677488327\n",
      "Epoch: 50 Test loss: 0.013107183389365673\n",
      "Epoch: 51 Test loss: 0.013095246627926826\n",
      "Epoch: 52 Test loss: 0.013087645173072815\n",
      "Epoch: 53 Test loss: 0.013094887137413025\n",
      "Epoch: 54 Test loss: 0.013105670921504498\n",
      "Epoch: 55 Test loss: 0.013076022267341614\n",
      "Epoch: 56 Test loss: 0.01308443583548069\n",
      "Epoch: 57 Test loss: 0.013094443827867508\n",
      "Epoch: 58 Test loss: 0.013095315545797348\n",
      "Epoch: 59 Test loss: 0.013101151213049889\n",
      "Epoch: 60 Test loss: 0.013096693903207779\n",
      "Epoch: 61 Test loss: 0.013127985410392284\n",
      "Epoch: 62 Test loss: 0.013071417808532715\n",
      "Epoch: 63 Test loss: 0.01309632696211338\n",
      "Epoch: 64 Test loss: 0.0130825936794281\n",
      "Epoch: 65 Test loss: 0.013086965307593346\n",
      "Epoch: 66 Test loss: 0.013102279976010323\n",
      "Epoch: 67 Test loss: 0.013088546693325043\n",
      "Epoch: 68 Test loss: 0.013088379055261612\n",
      "Epoch: 69 Test loss: 0.01308091264218092\n",
      "Epoch: 70 Test loss: 0.01309480331838131\n",
      "Epoch: 71 Test loss: 0.013093248941004276\n",
      "Epoch: 72 Test loss: 0.013094701804220676\n",
      "Epoch: 73 Test loss: 0.013090744614601135\n",
      "Epoch: 74 Test loss: 0.013082860969007015\n",
      "Epoch: 75 Test loss: 0.013086327351629734\n",
      "Epoch: 76 Test loss: 0.013091904111206532\n",
      "Epoch: 77 Test loss: 0.013085385784506798\n",
      "Epoch: 78 Test loss: 0.013089076615869999\n",
      "Epoch: 79 Test loss: 0.013089141808450222\n",
      "Epoch: 80 Test loss: 0.01309143751859665\n",
      "Epoch: 81 Test loss: 0.01308740396052599\n",
      "Epoch: 82 Test loss: 0.013085703365504742\n",
      "Epoch: 83 Test loss: 0.013088511303067207\n",
      "Epoch: 84 Test loss: 0.013089240528643131\n",
      "Epoch: 85 Test loss: 0.013085119426250458\n",
      "Epoch: 86 Test loss: 0.013087921775877476\n",
      "Epoch: 87 Test loss: 0.013088696636259556\n",
      "Epoch: 88 Test loss: 0.013088244944810867\n",
      "Epoch: 89 Test loss: 0.013087958097457886\n",
      "Epoch: 90 Test loss: 0.013088556937873363\n",
      "Epoch: 91 Test loss: 0.013088423758745193\n",
      "Epoch: 92 Test loss: 0.013088067062199116\n",
      "Epoch: 93 Test loss: 0.013088222593069077\n",
      "Epoch: 94 Test loss: 0.013087885454297066\n",
      "Epoch: 95 Test loss: 0.013087493367493153\n",
      "Epoch: 96 Test loss: 0.013087238185107708\n",
      "Epoch: 97 Test loss: 0.013087359257042408\n",
      "Epoch: 98 Test loss: 0.01308770664036274\n",
      "Epoch: 99 Test loss: 0.013087539002299309\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "epochs = 100\n",
    "early_stop_count = 0\n",
    "min_val_loss = float('inf')\n",
    "f = open(\"loss_Transformer.txt\", \"w\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        x_batch, y_batch = batch   \n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        \n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    scheduler.step(val_loss)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch: {epoch} Test loss: {val_losses[0]}\")\n",
    "        \n",
    "    # f.write(str(epoch) + '\\t' + f\"{float(loss):.6f}\" + '\\t' + f\"{float(test_loss):.6f}\" + '\\n')\n",
    "    \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fbe35f-4078-4ab9-9df6-dc4635c697ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
